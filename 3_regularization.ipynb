{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Deduplicated validation set (8093, 28, 28) (8093,)\n",
      "Deduplicated test set (7741, 28, 28) (7741,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  dedup_valid_dataset = save['dedup_valid_dataset']\n",
    "  dedup_valid_labels = save['dedup_valid_labels']\n",
    "  dedup_test_dataset = save['dedup_test_dataset']\n",
    "  dedup_test_labels = save['dedup_test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "  print('Deduplicated validation set', dedup_valid_dataset.shape, dedup_valid_labels.shape)\n",
    "  print('Deduplicated test set', dedup_test_dataset.shape, dedup_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n",
      "Deduplicated validation set (8093, 784) (8093, 10)\n",
      "Deduplicated test set (7741, 784) (7741, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.equal(np.arange(num_labels), labels[:,None])).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "orig_valid_dataset, orig_valid_labels = reformat(valid_dataset, valid_labels)\n",
    "orig_test_dataset, orig_test_labels = reformat(test_dataset, test_labels)\n",
    "dedup_valid_dataset, dedup_valid_labels = reformat(dedup_valid_dataset, dedup_valid_labels)\n",
    "dedup_test_dataset, dedup_test_labels = reformat(dedup_test_dataset, dedup_test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', orig_valid_dataset.shape, orig_valid_labels.shape)\n",
    "print('Test set', orig_test_dataset.shape, orig_test_labels.shape)\n",
    "print('Deduplicated validation set', dedup_valid_dataset.shape, dedup_valid_labels.shape)\n",
    "print('Deduplicated test set', dedup_test_dataset.shape, dedup_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dataset = dedup_valid_dataset\n",
    "valid_labels = dedup_valid_labels\n",
    "test_dataset = dedup_test_dataset\n",
    "test_labels = dedup_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dataset = orig_valid_dataset\n",
    "valid_labels = orig_valid_labels\n",
    "test_dataset = orig_test_dataset\n",
    "test_labels = orig_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.equal(np.argmax(predictions, 1), np.argmax(labels, 1)))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "l2_loss_coeff = 0.005\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = (tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    + l2_loss_coeff * tf.nn.l2_loss(weights))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 35.214622\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 8.4%\n",
      "Minibatch loss at step 500: 1.889750\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1000: 0.780084\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1500: 0.932671\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 2000: 0.596656\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2500: 0.626066\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 3000: 0.829144\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 3500: 0.869050\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 4000: 0.671679\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 4500: 0.849229\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 5000: 0.654628\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 5500: 0.722516\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 6000: 0.746610\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 6500: 0.879865\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 7000: 0.818349\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 7500: 0.652513\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 8000: 0.722485\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 8500: 0.624681\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 9000: 0.785852\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 9500: 0.670711\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 10000: 0.600722\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.2%\n",
      "Test accuracy: 87.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "l2_loss_coeff = 0.005\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    train_set = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    train_labs = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    valid_set = tf.constant(valid_dataset)\n",
    "    test_set = tf.constant(test_dataset)\n",
    "    \n",
    "    h1_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    h2_weights = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    h1_biases = tf.Variable(tf.zeros([hidden_size]))\n",
    "    h2_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def compute_logits(dataset):\n",
    "        h1_res = tf.nn.relu(tf.matmul(dataset, h1_weights) + h1_biases)\n",
    "        return tf.matmul(h1_res, h2_weights) + h2_biases\n",
    "        \n",
    "    logits = compute_logits(train_set)\n",
    "    \n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_labs, logits=logits))\n",
    "            + l2_loss_coeff * (tf.nn.l2_loss(h1_weights) + tf.nn.l2_loss(h2_weights)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_predictions = tf.nn.softmax(compute_logits(valid_set))\n",
    "    test_predictions = tf.nn.softmax(compute_logits(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Accuracy after step 0\n",
      "Loss: 1849.760010\n",
      "Train accuracy: 11.7%\n",
      "Valid accuracy: 29.6%\n",
      "Accuracy after step 100\n",
      "Loss: 970.021240\n",
      "Train accuracy: 76.6%\n",
      "Valid accuracy: 76.5%\n",
      "Accuracy after step 200\n",
      "Loss: 583.892761\n",
      "Train accuracy: 76.6%\n",
      "Valid accuracy: 77.5%\n",
      "Accuracy after step 300\n",
      "Loss: 350.385864\n",
      "Train accuracy: 78.9%\n",
      "Valid accuracy: 77.1%\n",
      "Accuracy after step 400\n",
      "Loss: 210.571640\n",
      "Train accuracy: 80.5%\n",
      "Valid accuracy: 78.6%\n",
      "Accuracy after step 500\n",
      "Loss: 127.205711\n",
      "Train accuracy: 81.2%\n",
      "Valid accuracy: 79.6%\n",
      "Accuracy after step 600\n",
      "Loss: 76.992065\n",
      "Train accuracy: 82.8%\n",
      "Valid accuracy: 82.0%\n",
      "Accuracy after step 700\n",
      "Loss: 46.914280\n",
      "Train accuracy: 81.2%\n",
      "Valid accuracy: 82.7%\n",
      "Accuracy after step 800\n",
      "Loss: 28.659807\n",
      "Train accuracy: 82.0%\n",
      "Valid accuracy: 83.7%\n",
      "Accuracy after step 900\n",
      "Loss: 17.560654\n",
      "Train accuracy: 85.2%\n",
      "Valid accuracy: 83.9%\n",
      "Accuracy after step 1000\n",
      "Loss: 10.885038\n",
      "Train accuracy: 85.2%\n",
      "Valid accuracy: 83.6%\n",
      "Accuracy after step 1100\n",
      "Loss: 6.858540\n",
      "Train accuracy: 89.1%\n",
      "Valid accuracy: 83.5%\n",
      "Accuracy after step 1200\n",
      "Loss: 4.507937\n",
      "Train accuracy: 82.8%\n",
      "Valid accuracy: 82.8%\n",
      "Accuracy after step 1300\n",
      "Loss: 2.965441\n",
      "Train accuracy: 83.6%\n",
      "Valid accuracy: 83.7%\n",
      "Accuracy after step 1400\n",
      "Loss: 1.883911\n",
      "Train accuracy: 87.5%\n",
      "Valid accuracy: 84.0%\n",
      "Accuracy after step 1500\n",
      "Loss: 1.610082\n",
      "Train accuracy: 84.4%\n",
      "Valid accuracy: 84.0%\n",
      "Accuracy after step 1600\n",
      "Loss: 1.142005\n",
      "Train accuracy: 85.2%\n",
      "Valid accuracy: 83.1%\n",
      "Accuracy after step 1700\n",
      "Loss: 0.889390\n",
      "Train accuracy: 87.5%\n",
      "Valid accuracy: 83.7%\n",
      "Accuracy after step 1800\n",
      "Loss: 0.695372\n",
      "Train accuracy: 90.6%\n",
      "Valid accuracy: 83.3%\n",
      "Accuracy after step 1900\n",
      "Loss: 0.674190\n",
      "Train accuracy: 85.2%\n",
      "Valid accuracy: 83.4%\n",
      "Accuracy after step 2000\n",
      "Loss: 0.616850\n",
      "Train accuracy: 89.1%\n",
      "Valid accuracy: 83.2%\n",
      "Final test accuracy: 89.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = step * batch_size % (len(train_dataset) - batch_size)\n",
    "        batch_set = train_dataset[offset:(offset+batch_size)]\n",
    "        batch_labs = train_labels[offset:(offset+batch_size)]\n",
    "        \n",
    "        feed_dict = { train_set : batch_set, train_labs : batch_labs }\n",
    "        _, l, train_p = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            print('Accuracy after step %d' % step)\n",
    "            print('Loss: %f' % l)\n",
    "            print('Train accuracy: %.1f%%' % accuracy(train_p, batch_labs))\n",
    "            print('Valid accuracy: %.1f%%' % accuracy(valid_predictions.eval(), valid_labels))\n",
    "        \n",
    "    print('Final test accuracy: %.1f%%' % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "l2_loss_coeff = 0.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    train_set = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    train_labs = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    valid_set = tf.constant(valid_dataset)\n",
    "    test_set = tf.constant(test_dataset)\n",
    "    \n",
    "    h1_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    h2_weights = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    h1_biases = tf.Variable(tf.zeros([hidden_size]))\n",
    "    h2_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def compute_logits(dataset):\n",
    "        h1_res = tf.nn.relu(tf.matmul(dataset, h1_weights) + h1_biases)\n",
    "        return tf.matmul(h1_res, h2_weights) + h2_biases\n",
    "        \n",
    "    logits = compute_logits(train_set)\n",
    "    \n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_labs, logits=logits))\n",
    "            + l2_loss_coeff * (tf.nn.l2_loss(h1_weights) + tf.nn.l2_loss(h2_weights)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_predictions = tf.nn.softmax(compute_logits(valid_set))\n",
    "    test_predictions = tf.nn.softmax(compute_logits(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Accuracy after step 0\n",
      "Loss: 333.970337\n",
      "Train accuracy: 10.9%\n",
      "Valid accuracy: 24.3%\n",
      "Accuracy after step 100\n",
      "Loss: 0.000003\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 200\n",
      "Loss: 0.000002\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 300\n",
      "Loss: 0.000002\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 400\n",
      "Loss: 0.000002\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 500\n",
      "Loss: 0.000001\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 600\n",
      "Loss: 0.000001\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 700\n",
      "Loss: 0.000001\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 800\n",
      "Loss: 0.000001\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 900\n",
      "Loss: 0.000001\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Accuracy after step 1000\n",
      "Loss: 0.000001\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 74.2%\n",
      "Final test accuracy: 81.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step % 5) * batch_size\n",
    "        batch_set = train_dataset[offset:(offset+batch_size)]\n",
    "        batch_labs = train_labels[offset:(offset+batch_size)]\n",
    "        \n",
    "        feed_dict = { train_set : batch_set, train_labs : batch_labs }\n",
    "        _, l, train_p = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            print('Accuracy after step %d' % step)\n",
    "            print('Loss: %f' % l)\n",
    "            print('Train accuracy: %.1f%%' % accuracy(train_p, batch_labs))\n",
    "            print('Valid accuracy: %.1f%%' % accuracy(valid_predictions.eval(), valid_labels))\n",
    "        \n",
    "    print('Final test accuracy: %.1f%%' % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "dropout_rate = 0.25\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    train_set = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    train_labs = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    valid_set = tf.constant(valid_dataset)\n",
    "    test_set = tf.constant(test_dataset)\n",
    "    \n",
    "    h1_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    h2_weights = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    h1_biases = tf.Variable(tf.zeros([hidden_size]))\n",
    "    h2_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def compute_logits(dataset, training=False):\n",
    "        h1_res = tf.nn.relu(tf.matmul(dataset, h1_weights) + h1_biases)\n",
    "        if training:\n",
    "            h1_res = tf.nn.dropout(h1_res, dropout_rate)\n",
    "        return tf.matmul(h1_res, h2_weights) + h2_biases\n",
    "        \n",
    "    logits = compute_logits(train_set, training=True)\n",
    "    \n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_labs, logits=logits)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_predictions = tf.nn.softmax(compute_logits(valid_set))\n",
    "    test_predictions = tf.nn.softmax(compute_logits(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Accuracy after step 0\n",
      "Loss: 649.715881\n",
      "Train accuracy: 7.0%\n",
      "Valid accuracy: 40.7%\n",
      "Accuracy after step 100\n",
      "Loss: 64.285126\n",
      "Train accuracy: 78.1%\n",
      "Valid accuracy: 77.4%\n",
      "Accuracy after step 200\n",
      "Loss: 36.085220\n",
      "Train accuracy: 87.5%\n",
      "Valid accuracy: 78.1%\n",
      "Accuracy after step 300\n",
      "Loss: 6.643983\n",
      "Train accuracy: 95.3%\n",
      "Valid accuracy: 78.3%\n",
      "Accuracy after step 400\n",
      "Loss: 30.625895\n",
      "Train accuracy: 86.7%\n",
      "Valid accuracy: 77.8%\n",
      "Accuracy after step 500\n",
      "Loss: 17.820826\n",
      "Train accuracy: 90.6%\n",
      "Valid accuracy: 78.4%\n",
      "Accuracy after step 600\n",
      "Loss: 6.656956\n",
      "Train accuracy: 96.1%\n",
      "Valid accuracy: 79.0%\n",
      "Accuracy after step 700\n",
      "Loss: 6.210125\n",
      "Train accuracy: 94.5%\n",
      "Valid accuracy: 79.0%\n",
      "Accuracy after step 800\n",
      "Loss: 11.989132\n",
      "Train accuracy: 92.2%\n",
      "Valid accuracy: 79.2%\n",
      "Accuracy after step 900\n",
      "Loss: 3.325966\n",
      "Train accuracy: 97.7%\n",
      "Valid accuracy: 78.6%\n",
      "Accuracy after step 1000\n",
      "Loss: 4.547933\n",
      "Train accuracy: 94.5%\n",
      "Valid accuracy: 78.9%\n",
      "Final test accuracy: 85.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step % 10) * batch_size\n",
    "        batch_set = train_dataset[offset:(offset+batch_size)]\n",
    "        batch_labs = train_labels[offset:(offset+batch_size)]\n",
    "        \n",
    "        feed_dict = { train_set : batch_set, train_labs : batch_labs }\n",
    "        _, l, train_p = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            print('Accuracy after step %d' % step)\n",
    "            print('Loss: %f' % l)\n",
    "            print('Train accuracy: %.1f%%' % accuracy(train_p, batch_labs))\n",
    "            print('Valid accuracy: %.1f%%' % accuracy(valid_predictions.eval(), valid_labels))\n",
    "        \n",
    "    print('Final test accuracy: %.1f%%' % accuracy(test_predictions.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "hidden_sizes = [1500, 500, 150, 50]\n",
    "dropout_keep_prob = 0.5\n",
    "l2_loss_coeff = 0.000\n",
    "num_steps = 20001\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    train_set = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    train_labs = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    valid_set = tf.constant(valid_dataset)\n",
    "    test_set = tf.constant(test_dataset)\n",
    "    \n",
    "    padded_hidden_sizes = [image_size * image_size] + hidden_sizes + [num_labels]\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(1, len(padded_hidden_sizes)):\n",
    "        stddev = math.sqrt(2.0 / padded_hidden_sizes[i - 1])\n",
    "        weights.append(tf.Variable(tf.truncated_normal([padded_hidden_sizes[i - 1], padded_hidden_sizes[i]], stddev=stddev)))\n",
    "        biases.append(tf.Variable(tf.zeros([padded_hidden_sizes[i]])))\n",
    "    \n",
    "    def compute_logits(dataset, training=False):\n",
    "        res = dataset\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            res = tf.nn.relu(tf.matmul(res, weights[i]) + biases[i])\n",
    "            if training:\n",
    "                res = tf.nn.dropout(res, dropout_keep_prob)\n",
    "        return tf.matmul(res, weights[-1]) + biases[-1]\n",
    "        \n",
    "    logits = compute_logits(train_set, training=False)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_labs, logits=logits))\n",
    "    for weight in weights:\n",
    "        loss = loss + l2_loss_coeff * tf.nn.l2_loss(weight)\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 500, 0.9)\n",
    "    #learning_rate = tf.train.polynomial_decay(0.5, global_step, num_steps)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_predictions = tf.nn.softmax(compute_logits(valid_set))\n",
    "    test_predictions = tf.nn.softmax(compute_logits(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Accuracy after step 0\n",
      "Loss: 2.338303\n",
      "Train accuracy: 16.0%\n",
      "Valid accuracy: 25.2%\n",
      "Accuracy after step 100\n",
      "Loss: 0.588078\n",
      "Train accuracy: 83.0%\n",
      "Valid accuracy: 79.3%\n",
      "Accuracy after step 200\n",
      "Loss: 0.520857\n",
      "Train accuracy: 84.0%\n",
      "Valid accuracy: 83.5%\n",
      "Accuracy after step 300\n",
      "Loss: 0.591900\n",
      "Train accuracy: 85.0%\n",
      "Valid accuracy: 83.1%\n",
      "Accuracy after step 400\n",
      "Loss: 0.618255\n",
      "Train accuracy: 81.0%\n",
      "Valid accuracy: 84.6%\n",
      "Accuracy after step 500\n",
      "Loss: 0.620643\n",
      "Train accuracy: 79.0%\n",
      "Valid accuracy: 85.1%\n",
      "Accuracy after step 600\n",
      "Loss: 0.607025\n",
      "Train accuracy: 82.0%\n",
      "Valid accuracy: 85.6%\n",
      "Accuracy after step 700\n",
      "Loss: 0.379936\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 86.2%\n",
      "Accuracy after step 800\n",
      "Loss: 0.653869\n",
      "Train accuracy: 82.0%\n",
      "Valid accuracy: 85.5%\n",
      "Accuracy after step 900\n",
      "Loss: 0.496533\n",
      "Train accuracy: 85.0%\n",
      "Valid accuracy: 86.4%\n",
      "Accuracy after step 1000\n",
      "Loss: 0.542236\n",
      "Train accuracy: 82.0%\n",
      "Valid accuracy: 87.0%\n",
      "Accuracy after step 1100\n",
      "Loss: 0.428788\n",
      "Train accuracy: 83.0%\n",
      "Valid accuracy: 86.8%\n",
      "Accuracy after step 1200\n",
      "Loss: 0.290370\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 87.0%\n",
      "Accuracy after step 1300\n",
      "Loss: 0.393412\n",
      "Train accuracy: 88.0%\n",
      "Valid accuracy: 87.3%\n",
      "Accuracy after step 1400\n",
      "Loss: 0.376120\n",
      "Train accuracy: 89.0%\n",
      "Valid accuracy: 87.5%\n",
      "Accuracy after step 1500\n",
      "Loss: 0.580730\n",
      "Train accuracy: 78.0%\n",
      "Valid accuracy: 87.1%\n",
      "Accuracy after step 1600\n",
      "Loss: 0.311162\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 87.9%\n",
      "Accuracy after step 1700\n",
      "Loss: 0.459412\n",
      "Train accuracy: 85.0%\n",
      "Valid accuracy: 87.9%\n",
      "Accuracy after step 1800\n",
      "Loss: 0.679020\n",
      "Train accuracy: 77.0%\n",
      "Valid accuracy: 88.4%\n",
      "Accuracy after step 1900\n",
      "Loss: 0.265375\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 88.1%\n",
      "Accuracy after step 2000\n",
      "Loss: 0.380564\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 88.3%\n",
      "Accuracy after step 2100\n",
      "Loss: 0.333436\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 88.7%\n",
      "Accuracy after step 2200\n",
      "Loss: 0.304148\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 88.5%\n",
      "Accuracy after step 2300\n",
      "Loss: 0.298806\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 88.9%\n",
      "Accuracy after step 2400\n",
      "Loss: 0.399396\n",
      "Train accuracy: 87.0%\n",
      "Valid accuracy: 88.5%\n",
      "Accuracy after step 2500\n",
      "Loss: 0.179201\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 89.0%\n",
      "Accuracy after step 2600\n",
      "Loss: 0.288002\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 88.4%\n",
      "Accuracy after step 2700\n",
      "Loss: 0.282394\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 88.9%\n",
      "Accuracy after step 2800\n",
      "Loss: 0.396064\n",
      "Train accuracy: 89.0%\n",
      "Valid accuracy: 89.1%\n",
      "Accuracy after step 2900\n",
      "Loss: 0.381941\n",
      "Train accuracy: 88.0%\n",
      "Valid accuracy: 88.6%\n",
      "Accuracy after step 3000\n",
      "Loss: 0.406287\n",
      "Train accuracy: 88.0%\n",
      "Valid accuracy: 89.1%\n",
      "Accuracy after step 3100\n",
      "Loss: 0.272247\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 89.2%\n",
      "Accuracy after step 3200\n",
      "Loss: 0.420112\n",
      "Train accuracy: 84.0%\n",
      "Valid accuracy: 89.4%\n",
      "Accuracy after step 3300\n",
      "Loss: 0.239367\n",
      "Train accuracy: 93.0%\n",
      "Valid accuracy: 89.4%\n",
      "Accuracy after step 3400\n",
      "Loss: 0.315207\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 89.1%\n",
      "Accuracy after step 3500\n",
      "Loss: 0.319538\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 89.3%\n",
      "Accuracy after step 3600\n",
      "Loss: 0.310010\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 89.7%\n",
      "Accuracy after step 3700\n",
      "Loss: 0.326326\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 89.4%\n",
      "Accuracy after step 3800\n",
      "Loss: 0.225705\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 89.5%\n",
      "Accuracy after step 3900\n",
      "Loss: 0.197770\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 89.3%\n",
      "Accuracy after step 4000\n",
      "Loss: 0.288550\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 89.7%\n",
      "Accuracy after step 4100\n",
      "Loss: 0.411264\n",
      "Train accuracy: 88.0%\n",
      "Valid accuracy: 89.6%\n",
      "Accuracy after step 4200\n",
      "Loss: 0.297738\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 89.9%\n",
      "Accuracy after step 4300\n",
      "Loss: 0.337935\n",
      "Train accuracy: 87.0%\n",
      "Valid accuracy: 89.9%\n",
      "Accuracy after step 4400\n",
      "Loss: 0.234694\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 89.9%\n",
      "Accuracy after step 4500\n",
      "Loss: 0.262514\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 89.9%\n",
      "Accuracy after step 4600\n",
      "Loss: 0.264253\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 89.6%\n",
      "Accuracy after step 4700\n",
      "Loss: 0.220945\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 89.9%\n",
      "Accuracy after step 4800\n",
      "Loss: 0.151867\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.0%\n",
      "Accuracy after step 4900\n",
      "Loss: 0.254845\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 89.9%\n",
      "Accuracy after step 5000\n",
      "Loss: 0.284662\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 90.3%\n",
      "Accuracy after step 5100\n",
      "Loss: 0.392592\n",
      "Train accuracy: 86.0%\n",
      "Valid accuracy: 89.9%\n",
      "Accuracy after step 5200\n",
      "Loss: 0.229489\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 90.2%\n",
      "Accuracy after step 5300\n",
      "Loss: 0.285284\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 90.1%\n",
      "Accuracy after step 5400\n",
      "Loss: 0.202052\n",
      "Train accuracy: 93.0%\n",
      "Valid accuracy: 90.0%\n",
      "Accuracy after step 5500\n",
      "Loss: 0.235889\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.0%\n",
      "Accuracy after step 5600\n",
      "Loss: 0.145502\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.3%\n",
      "Accuracy after step 5700\n",
      "Loss: 0.253763\n",
      "Train accuracy: 93.0%\n",
      "Valid accuracy: 90.3%\n",
      "Accuracy after step 5800\n",
      "Loss: 0.162930\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 90.2%\n",
      "Accuracy after step 5900\n",
      "Loss: 0.148455\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.2%\n",
      "Accuracy after step 6000\n",
      "Loss: 0.206788\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.3%\n",
      "Accuracy after step 6100\n",
      "Loss: 0.265525\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 90.3%\n",
      "Accuracy after step 6200\n",
      "Loss: 0.219853\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.5%\n",
      "Accuracy after step 6300\n",
      "Loss: 0.379144\n",
      "Train accuracy: 89.0%\n",
      "Valid accuracy: 90.5%\n",
      "Accuracy after step 6400\n",
      "Loss: 0.154713\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.4%\n",
      "Accuracy after step 6500\n",
      "Loss: 0.112575\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.5%\n",
      "Accuracy after step 6600\n",
      "Loss: 0.326996\n",
      "Train accuracy: 93.0%\n",
      "Valid accuracy: 90.1%\n",
      "Accuracy after step 6700\n",
      "Loss: 0.151196\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.5%\n",
      "Accuracy after step 6800\n",
      "Loss: 0.136185\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.5%\n",
      "Accuracy after step 6900\n",
      "Loss: 0.124118\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.1%\n",
      "Accuracy after step 7000\n",
      "Loss: 0.132226\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.5%\n",
      "Accuracy after step 7100\n",
      "Loss: 0.248984\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 90.4%\n",
      "Accuracy after step 7200\n",
      "Loss: 0.188319\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 90.7%\n",
      "Accuracy after step 7300\n",
      "Loss: 0.309761\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 90.5%\n",
      "Accuracy after step 7400\n",
      "Loss: 0.132026\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 7500\n",
      "Loss: 0.120310\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 7600\n",
      "Loss: 0.310338\n",
      "Train accuracy: 90.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 7700\n",
      "Loss: 0.140120\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 7800\n",
      "Loss: 0.156579\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 7900\n",
      "Loss: 0.186888\n",
      "Train accuracy: 93.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 8000\n",
      "Loss: 0.099713\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 8100\n",
      "Loss: 0.202799\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 90.7%\n",
      "Accuracy after step 8200\n",
      "Loss: 0.103279\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 8300\n",
      "Loss: 0.188870\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.7%\n",
      "Accuracy after step 8400\n",
      "Loss: 0.216825\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 8500\n",
      "Loss: 0.293297\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 90.7%\n",
      "Accuracy after step 8600\n",
      "Loss: 0.029482\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 8700\n",
      "Loss: 0.322120\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 90.6%\n",
      "Accuracy after step 8800\n",
      "Loss: 0.140279\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.7%\n",
      "Accuracy after step 8900\n",
      "Loss: 0.228641\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.7%\n",
      "Accuracy after step 9000\n",
      "Loss: 0.170172\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 9100\n",
      "Loss: 0.110455\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 9200\n",
      "Loss: 0.153308\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 9300\n",
      "Loss: 0.227180\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 90.7%\n",
      "Accuracy after step 9400\n",
      "Loss: 0.215958\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 9500\n",
      "Loss: 0.188326\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 9600\n",
      "Loss: 0.149725\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 9700\n",
      "Loss: 0.229085\n",
      "Train accuracy: 93.0%\n",
      "Valid accuracy: 90.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after step 9800\n",
      "Loss: 0.179338\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 9900\n",
      "Loss: 0.192461\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 10000\n",
      "Loss: 0.155192\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 10100\n",
      "Loss: 0.082076\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 10200\n",
      "Loss: 0.146969\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.2%\n",
      "Accuracy after step 10300\n",
      "Loss: 0.145314\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 10400\n",
      "Loss: 0.081979\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 10500\n",
      "Loss: 0.239343\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 10600\n",
      "Loss: 0.118128\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 10700\n",
      "Loss: 0.251653\n",
      "Train accuracy: 91.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 10800\n",
      "Loss: 0.153650\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 10900\n",
      "Loss: 0.086768\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 11000\n",
      "Loss: 0.161007\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 11100\n",
      "Loss: 0.117534\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 11200\n",
      "Loss: 0.097566\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 90.8%\n",
      "Accuracy after step 11300\n",
      "Loss: 0.161229\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 11400\n",
      "Loss: 0.073122\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 11500\n",
      "Loss: 0.116407\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 11600\n",
      "Loss: 0.111226\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 11700\n",
      "Loss: 0.205856\n",
      "Train accuracy: 93.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 11800\n",
      "Loss: 0.105755\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 11900\n",
      "Loss: 0.115976\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 12000\n",
      "Loss: 0.037078\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 12100\n",
      "Loss: 0.133228\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 12200\n",
      "Loss: 0.105615\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 12300\n",
      "Loss: 0.241809\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 12400\n",
      "Loss: 0.081610\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 12500\n",
      "Loss: 0.089282\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 12600\n",
      "Loss: 0.149122\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 12700\n",
      "Loss: 0.101495\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 12800\n",
      "Loss: 0.118940\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 12900\n",
      "Loss: 0.163136\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 13000\n",
      "Loss: 0.090888\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 13100\n",
      "Loss: 0.102221\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 13200\n",
      "Loss: 0.124254\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 13300\n",
      "Loss: 0.128476\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 13400\n",
      "Loss: 0.142237\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 13500\n",
      "Loss: 0.107254\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 13600\n",
      "Loss: 0.136418\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 13700\n",
      "Loss: 0.150893\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 13800\n",
      "Loss: 0.097173\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 13900\n",
      "Loss: 0.142442\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 14000\n",
      "Loss: 0.107416\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 14100\n",
      "Loss: 0.104242\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 14200\n",
      "Loss: 0.153712\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 14300\n",
      "Loss: 0.106476\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 14400\n",
      "Loss: 0.133135\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.2%\n",
      "Accuracy after step 14500\n",
      "Loss: 0.099643\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 14600\n",
      "Loss: 0.023491\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 14700\n",
      "Loss: 0.215338\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 14800\n",
      "Loss: 0.096216\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 14900\n",
      "Loss: 0.077012\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 15000\n",
      "Loss: 0.078319\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 15100\n",
      "Loss: 0.097111\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 15200\n",
      "Loss: 0.098234\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 15300\n",
      "Loss: 0.116684\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 15400\n",
      "Loss: 0.037172\n",
      "Train accuracy: 99.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 15500\n",
      "Loss: 0.028032\n",
      "Train accuracy: 99.0%\n",
      "Valid accuracy: 91.2%\n",
      "Accuracy after step 15600\n",
      "Loss: 0.138742\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 15700\n",
      "Loss: 0.105948\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 15800\n",
      "Loss: 0.052068\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 15900\n",
      "Loss: 0.160874\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 16000\n",
      "Loss: 0.199568\n",
      "Train accuracy: 92.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 16100\n",
      "Loss: 0.074939\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 16200\n",
      "Loss: 0.079174\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 16300\n",
      "Loss: 0.111807\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 16400\n",
      "Loss: 0.085173\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 16500\n",
      "Loss: 0.111283\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.2%\n",
      "Accuracy after step 16600\n",
      "Loss: 0.009763\n",
      "Train accuracy: 100.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 16700\n",
      "Loss: 0.139941\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 16800\n",
      "Loss: 0.165987\n",
      "Train accuracy: 94.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 16900\n",
      "Loss: 0.081273\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 17000\n",
      "Loss: 0.082672\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 17100\n",
      "Loss: 0.024413\n",
      "Train accuracy: 99.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 17200\n",
      "Loss: 0.096592\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 17300\n",
      "Loss: 0.092205\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 17400\n",
      "Loss: 0.088280\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 17500\n",
      "Loss: 0.151742\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 17600\n",
      "Loss: 0.057201\n",
      "Train accuracy: 99.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 17700\n",
      "Loss: 0.092747\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 17800\n",
      "Loss: 0.065244\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 17900\n",
      "Loss: 0.078339\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 18000\n",
      "Loss: 0.052380\n",
      "Train accuracy: 99.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 18100\n",
      "Loss: 0.059017\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 18200\n",
      "Loss: 0.119549\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.2%\n",
      "Accuracy after step 18300\n",
      "Loss: 0.148094\n",
      "Train accuracy: 95.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 18400\n",
      "Loss: 0.125085\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 18500\n",
      "Loss: 0.145371\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 18600\n",
      "Loss: 0.102222\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 18700\n",
      "Loss: 0.083285\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 18800\n",
      "Loss: 0.040023\n",
      "Train accuracy: 99.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 18900\n",
      "Loss: 0.116258\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 19000\n",
      "Loss: 0.114232\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 19100\n",
      "Loss: 0.106110\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 90.9%\n",
      "Accuracy after step 19200\n",
      "Loss: 0.105337\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 19300\n",
      "Loss: 0.099396\n",
      "Train accuracy: 96.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 19400\n",
      "Loss: 0.076037\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after step 19500\n",
      "Loss: 0.079373\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 19600\n",
      "Loss: 0.072904\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 19700\n",
      "Loss: 0.078256\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 19800\n",
      "Loss: 0.048843\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.1%\n",
      "Accuracy after step 19900\n",
      "Loss: 0.106828\n",
      "Train accuracy: 98.0%\n",
      "Valid accuracy: 91.0%\n",
      "Accuracy after step 20000\n",
      "Loss: 0.123958\n",
      "Train accuracy: 97.0%\n",
      "Valid accuracy: 91.1%\n",
      "stopping early after 20000 steps\n",
      "Final test accuracy: 96.4%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    prev_valid_acc = 0.0\n",
    "    prev_test_acc = 0.0\n",
    "    final_test_acc = None\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = step * batch_size % (len(train_dataset) - batch_size)\n",
    "        batch_set = train_dataset[offset:(offset+batch_size)]\n",
    "        batch_labs = train_labels[offset:(offset+batch_size)]\n",
    "        \n",
    "        feed_dict = { train_set : batch_set, train_labs : batch_labs }\n",
    "        _, l, train_p = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    \n",
    "        valid_acc = None\n",
    "        if (step % 100 == 0):\n",
    "            valid_acc = accuracy(valid_predictions.eval(), valid_labels)\n",
    "            \n",
    "            print('Accuracy after step %d' % step)\n",
    "            print('Loss: %f' % l)\n",
    "            print('Train accuracy: %.1f%%' % accuracy(train_p, batch_labs))\n",
    "            print('Valid accuracy: %.1f%%' % valid_acc)\n",
    "        \n",
    "        if (step % 2000 == 0):\n",
    "            if valid_acc < prev_valid_acc:\n",
    "                print('stopping early after %d steps' % step)\n",
    "                final_test_acc = prev_test_acc\n",
    "                #break\n",
    "            else:\n",
    "                prev_valid_acc = valid_acc\n",
    "                prev_test_acc = accuracy(test_predictions.eval(), test_labels)\n",
    "                \n",
    "    if final_test_acc == None:\n",
    "        final_test_acc = accuracy(test_predictions.eval(), test_labels)\n",
    "    print('Final test accuracy: %.1f%%' % final_test_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
